# ğŸ¤– Ollama RAG â€” Document Question Answering App

This project is a simple **Retrieval-Augmented Generation (RAG)** system built with **LangChain**, **Ollama**, and **FAISS**.  
It allows users to upload a PDF file ğŸ“„ and ask natural language questions ğŸ’¬ â€” the app retrieves relevant document chunks and generates intelligent answers using a local LLM.

---

## ğŸš€ Features
âœ… Upload any PDF file  
âœ… Ask questions about its content  
âœ… Uses local **Ollama LLM** (`llama3:instruct`)  
âœ… Performs semantic retrieval with **FAISS**  
âœ… Clean and simple **Streamlit UI**

---

## ğŸ§© Tech Stack
| Component | Description |
|------------|--------------|
| ğŸ§  **Ollama** | Local Large Language Model (LLM) |
| ğŸ§¾ **LangChain** | Manages document splitting, retrieval, and chaining |
| ğŸ’¬ **Streamlit** | Frontend interface |
| ğŸ” **FAISS** | Vector similarity search |
| ğŸ§© **HuggingFace Embeddings** | Text embedding model |
| ğŸ“„ **Unstructured** | Document loader for PDF/Text |

---

## âš™ï¸ Project Structure

ollama-rag/
â”‚
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ doc_chat_utility.py # RAG logic (retrieval + generation)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project description


---

## ğŸ§  How It Works
1. **Upload a file** (PDF or text-based).  
2. The app splits the document into small chunks.  
3. Each chunk is embedded using **HuggingFace embeddings**.  
4. **FAISS** stores and retrieves the most relevant chunks based on the query.  
5. **Ollama (llama3:instruct)** generates a natural language answer.

---

## ğŸ–¥ï¸ Installation & Running Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<moustafaaaaaa>/ollama-rag.git
cd ollama-rag

2ï¸âƒ£ Create and activate a virtual environment
python -m venv venv
venv\Scripts\activate     # On Windows
# OR
source venv/bin/activate  # On macOS/Linux

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Make sure Ollama is running locally

Download Ollama and run the LLaMA 3 model:

ollama run llama3:instruct

5ï¸âƒ£ Run the app
streamlit run app.py


Then open your browser and go to ğŸ‘‰ http://localhost:8501
